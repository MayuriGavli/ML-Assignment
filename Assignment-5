1. What are the key tasks that machine learning entails? What does data pre-processing imply?

  Machine Learning (ML) involves several core tasks across its lifecycle. These include:
  
  a. Problem Definition
  Clearly define what problem you're trying to solve (e.g., classification, regression, clustering).
  Understand the goals, constraints, and success criteria.
  b. Data Collection
  Gather raw data from sources such as databases, APIs, sensors, or web scraping.
  c. Data Preprocessing
  Clean and prepare the data (more detail below).
  Handle missing values, outliers, duplicates, etc.
  d. Exploratory Data Analysis (EDA)
  Analyze the structure, trends, and patterns in the data.
  Use visualizations and statistics to understand variable relationships.
  e. Feature Engineering
  Create, modify, or select features (variables) to improve model performance.
  Convert categorical variables to numerical ones, normalize data, etc.
  f. Model Selection
  Choose an appropriate algorithm (e.g., decision tree, SVM, neural network) based on the task and data type.
  g. Model Training
  Train the model on the training data.
  Adjust weights/parameters based on loss functions.
  h. Model Evaluation
  Test the model using metrics like accuracy, precision, recall, RMSE, etc.
  Validate using cross-validation or test datasets.
  i. Hyperparameter Tuning
  Optimize model parameters (e.g., learning rate, depth of trees) using grid search or random search.
  j. Model Deployment
  Integrate the trained model into a production system (e.g., web app, API).
  k. Monitoring and Maintenance
  Monitor the model‚Äôs performance over time and retrain when needed due to data drift or concept drift.
  
  Data preprocessing is a crucial step in the ML pipeline that involves transforming raw data into a usable format for modeling. It includes:
  
  a. Data Cleaning
  Handle missing values (e.g., impute with mean/median or drop).
  Remove duplicates.
  Correct inconsistent formats or typos.
  b. Data Transformation
  Normalization/Standardization: Scale numeric features to bring them to a common range.
  Encoding Categorical Variables:
  One-hot encoding
  Label encoding
  c. Feature Extraction & Selection
  Remove irrelevant or redundant features.
  Create new features (e.g., from dates or text).
  Select important features based on correlation or model importance.
  d. Handling Outliers
  Identify outliers using box plots, Z-scores, or IQR.
  Choose to remove or cap/floor them depending on impact.
  e. Data Splitting
  Divide the dataset into training, validation, and test sets.
  f. Dealing with Imbalanced Data
  Use techniques like SMOTE, undersampling, or class weighting if one class dominates.
 ..................................................................................
2. Describe quantitative and qualitative data in depth. Make a distinction between the two.

  Quantitative data refers to data that can be measured and expressed numerically. It represents quantities and allows mathematical operations like addition, subtraction, and averaging.
  Characteristics: Numerical , Objective and measurable , Can be used to generate statistics and graphs
  üìö Types of Quantitative Data:
  Discrete Data: Countable numbers , No decimals or fractions , Example: Number of students in a class (e.g., 25)
  Continuous Data: Can take any value within a range , Includes decimals and fractions , Example: Height of students (e.g., 5.8 feet)

  Qualitative Data (also called categorical data) refers to non-numeric data that describes qualities or categories. It captures attributes, labels, or classifications.
  Characteristics: Cannot be measured numerically , Usually grouped into categories
  üìö Types of Qualitative Data:
  Nominal Data: Categories with no natural order , Example: Blood type (A, B, AB, O)
  Ordinal Data: Categories with a meaningful order but not measurable distances , Example: Education level (High School < Bachelor < Master < PhD)

| Feature         | Quantitative Data                      | Qualitative Data                        |
| --------------- | -------------------------------------- | --------------------------------------- |
| **Nature**      | Numerical                              | Categorical / Descriptive               |
| **Measurement** | Measurable in units                    | Not measured, just labeled              |
| **Operations**  | Can perform math operations            | Cannot do math (only counting/grouping) |
| **Types**       | Discrete, Continuous                   | Nominal, Ordinal                        |
| **Examples**    | Age, Salary, Height                    | Gender, Nationality, Feedback           |
| **Use in ML**   | Features for regression/classification | Often encoded for models                |

..................................................................................

3. Create a basic data collection that includes some sample records. Have at least one attribute from each of the machine learning data types.

| Attribute Name          | Example Value       | ML Data Type                     | Explanation                      |
| ----------------------- | ------------------- | -------------------------------- | -------------------------------- |
| **Customer\_ID**        | C001                | Identifier (ignored in training) | Unique ID, not used for learning |
| **Age**                 | 24                  | Quantitative (Numeric)           | Continuous variable              |
| **Gender**              | Male, Female, Other | Qualitative (Nominal)            | No natural order                 |
| **Purchase\_Amount**    | 1250.50             | Quantitative (Numeric)           | Continuous variable              |
| **Satisfaction\_Level** | High, Medium, Low   | Qualitative (Ordinal)            | Has logical order                |
| **Purchased\_Category** | Electronics         | Qualitative (Nominal)            | Categorical data                 |
| **Feedback\_Text**      | "Great product..."  | Text (Unstructured)              | Needs NLP techniques to use      |

..................................................................................

4. What are the various causes of machine learning data issues? What are the ramifications?

| Data Issue           | Ramification/Impact                                                        |
| -------------------- | -------------------------------------------------------------------------- |
| Missing values       | Model may fail to train or give biased results                             |
| Noisy/outlier data   | Poor model generalization, increased error                                 |
| Duplicate records    | Skewed statistics and overfitting                                          |
| Format inconsistency | Errors during data loading, preprocessing, or analysis                     |
| Class imbalance      | Biased predictions toward the majority class                               |
| Biased data          | Discriminatory results, ethical concerns, regulatory risks                 |
| Outdated data        | Irrelevant predictions, business losses                                    |
| Irrelevant features  | Increased training time, reduced model accuracy                            |
| Incorrect labels     | Misleading training, model confusion                                       |
| Data leakage         | Unrealistically high accuracy during training, poor real-world performance |

..................................................................................

5. Demonstrate various approaches to categorical data exploration with appropriate examples.
  1. Frequency Distribution
  import pandas as pd
  data = pd.DataFrame({
    'Department': ['Sales', 'HR', 'Sales', 'IT', 'HR', 'IT', 'Sales']
  })
  print(data['Department'].value_counts())

  2. Bar Plot
  import seaborn as sns
  import matplotlib.pyplot as plt
  sns.countplot(x='Department', data=data)
  plt.title("Department Distribution")
  plt.show()

  3. Pie Chart
  data['Department'].value_counts().plot.pie(autopct='%1.1f%%')
  plt.title("Department Share")
  plt.ylabel('')
  plt.show()

..................................................................................

6. How would the learning activity be affected if certain variables have missing values? Having said that, what can be done about it?

  When certain variables have missing values, it can significantly affect the learning process, model accuracy, and generalization. Here's how:
  1. Training Failures
  Many ML algorithms cannot handle missing values directly and will throw errors.
  2. Biased Learning
  If missing values are unevenly distributed (e.g., mostly from one group), models may learn biased patterns.  
  3. Loss of Information
  Removing rows with missing data reduces the amount of training data, especially harmful in small datasets.
  4. Incorrect Statistics
  Mean, variance, correlation, etc., become misleading when calculated on incomplete data.
  5. Skewed Feature Importance
  Features with missing values may appear less useful to the model, even if they are important when complete.

  | Strategy                                         | Description                                                    | When to Use                          |
| ------------------------------------------------ | -------------------------------------------------------------- | ------------------------------------ |
| **1. Remove rows**                               | Drop rows with missing values                                  | When missing <5% and data is large   |
| **2. Remove columns**                            | Drop columns with too many missing values                      | When column is >50% missing          |
| **3. Imputation (mean/median)**                  | Fill missing values with average or middle value of the column | For numeric, symmetric distributions |
| **4. Mode imputation**                           | Use most frequent value                                        | For categorical data                 |
| **5. Predictive imputation**                     | Use models (e.g., KNN, regression) to estimate missing values  | When high accuracy is needed         |
| **6. Constant or "Unknown"**                     | Fill with a placeholder (e.g., "Unknown")                      | For nominal categorical variables    |
| **7. Use algorithms that handle missing values** | Like XGBoost or LightGBM                                       | When you prefer model-side handling  |
| **8. Indicator variable**                        | Add a new feature to flag missingness                          | When missingness itself carries info |

..................................................................................

7. Describe the various methods for dealing with missing data values in depth.

1. Listwise Deletion (Removing Rows) :- Remove any row that contains one or more missing values
Ex - df.dropna(inplace=True)

2. Column Deletion :- Drop columns that have too many missing values.
Ex - df.drop(columns=['ColumnName'], inplace=True)

3. Mean/Median Imputation :- Replace missing values with the mean (for symmetric data) or median (for skewed data).
Ex - from sklearn.impute import SimpleImputer
     imputer = SimpleImputer(strategy='mean')  # or 'median'
     df['Age'] = imputer.fit_transform(df[['Age']])

4. Mode Imputation :- Fill missing values with the most frequent (mode) value.
Ex - df['Gender'] = df['Gender'].fillna(df['Gender'].mode()[0])

5. Constant/Custom Value Imputation :- Replace missing values with a constant value, like 0, Unknown, or -1.
Ex - df['City'] = df['City'].fillna('Unknown')

..................................................................................

8. What are the various data pre-processing techniques? Explain dimensionality reduction and function selection in a few words.
  
  Common Data Preprocessing Techniques
  1. Data Cleaning
  Handle missing values (impute or remove)
  Remove duplicates
  Correct inconsistent formatting
  Handle outliers
  2. Data Transformation
  Normalize or standardize numerical data
  Log or power transformation to handle skewed data
  Encode categorical data (Label Encoding, One-Hot Encoding)
  3. Feature Engineering
  Create new features from existing ones (e.g., extract year from date)
  Binning (e.g., age groups)
  Text vectorization (TF-IDF, Bag of Words)
  4. Feature Scaling
  Normalization: Scales data to a [0,1] range (MinMaxScaler)
  Standardization: Mean = 0, SD = 1 (StandardScaler)
  These are required for ML algorithms sensitive to magnitude (e.g., KNN, SVM, Logistic Regression)
  5. Encoding Categorical Variables
  Label Encoding: Converts categories to integers
  One-Hot Encoding: Creates binary columns for each category
  6. Outlier Handling
  Z-score, IQR methods to detect outliers
  Remove or cap values beyond threshold
  7. Data Splitting
  Split into training, validation, and test sets

  
  Dimensionality Reduction :- Reducing the number of features (dimensions) while preserving the essence or variance of the data.
  use it  to :-  Avoid overfitting , Improve model speed , Reduce noise , Visualize high-dimensional data 
  Common Techniques: PCA

  Feature Selection :- Choosing the most relevant features from the dataset for the model ‚Äî not reducing dimensions but eliminating irrelevant or redundant ones.
  Benefits:- Improves model performance , Reduces overfitting

..................................................................................

9.i. What is the IQR? What criteria are used to assess it?

IQR = Interquartile Range :- The Interquartile Range (IQR) is a measure of statistical dispersion. It shows the range of the middle 50% of a dataset, and it helps identify how spread out values are.
Formula: IQR = Q3 ‚àí Q1
Q1 (1st Quartile): 25th percentile (25% of values lie below this point)
Q3 (3rd Quartile): 75th percentile (75% of values lie below this point)
It is Assessed :- Detect outliers , Understand data spread , Compare variability across datasets

Outlier Detection Criteria: An observation is considered an outlier if it falls: Below: Q1 ‚àí 1.5 √ó IQR , Above: Q3 + 1.5 √ó IQR

ii. Describe the various components of a box plot in detail? When will the lower whisker surpass the upper whisker in length? How can box plots be used to identify outliers?
A box plot (or box-and-whisker plot) visually summarizes a dataset using 5-number statistics.

| Component             | Description                                          |
| --------------------- | ---------------------------------------------------- |
| **Minimum**           | Lowest value (excluding outliers)                    |
| **Q1 (1st Quartile)** | 25th percentile                                      |
| **Median (Q2)**       | 50th percentile (middle of the data)                 |
| **Q3 (3rd Quartile)** | 75th percentile                                      |
| **Maximum**           | Highest value (excluding outliers)                   |
| **Whiskers**          | Lines from box to min/max within the 1.5 √ó IQR range |
| **Outliers**          | Points plotted individually outside whiskers         |

 When Does the Lower Whisker Surpass the Upper Whisker in Length?
  This happens when the data is negatively skewed (left-skewed):
  The lower half of the data is more spread out.
  So, Q1 ‚àí Min > Max ‚àí Q3, making the lower whisker longer.

 How Can Box Plots Identify Outliers?
  Any data point beyond:
  Q1 ‚àí 1.5 √ó IQR or
  Q3 + 1.5 √ó IQR
  Will appear as dots or stars beyond the whiskers in the plot.

10. Make brief notes on any two of the following:

1Ô∏è‚É£ Data Collected at Regular Intervals (Time Series Data)
This is sequential data recorded over uniform time gaps (e.g., hourly, daily, monthly).

Example: Daily stock prices, monthly sales.

Used in trend analysis, forecasting, and seasonal pattern detection.

Requires special techniques (e.g., ARIMA, LSTM).

2Ô∏è‚É£ The Gap Between the Quartiles (IQR)
Known as the Interquartile Range (IQR).

Formula: IQR = Q3 ‚àí Q1

Represents the spread of the middle 50% of data.

Used to identify outliers and measure data dispersion.


1. Data with nominal and ordinal values
| Feature          | Nominal              | Ordinal                                |
| ---------------- | -------------------- | -------------------------------------- |
| Meaning          | Labels or categories | Categories with a **meaningful order** |
| Order            | No order             | Has a natural order                    |
| Example          | Gender, Country      | Satisfaction level (Low < Med < High)  |
| Numeric relation | Not meaningful       | Rank is meaningful                     |


2. Histogram and box plot
| Metric               | Mean (Average)                  | Median                       |
| -------------------- | ------------------------------- | ---------------------------- |
| Definition           | Sum of all values / total count | Middle value of sorted data  |
| Affected by outliers | Yes, highly                     | No, robust to outliers       |
| Best for             | Symmetric data                  | Skewed or data with outliers |
| Example              | Mean of \[1, 2, 100] = 34.33    | Median of \[1, 2, 100] = 2   |


